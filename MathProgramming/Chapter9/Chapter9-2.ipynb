{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter9-2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m7L3_Mk4UIUy"},"source":["# 第9章 深層学習によって時系列を扱う仕組みを知ろう(6-10節)\n","ここでは、RNNとCNNによる時系列データの学習について学んでいきます。  \n","\n","Google Colaboratory上で実行する場合、ランタイムがGPUになっていることを確認して下さい\n"]},{"cell_type":"code","metadata":{"id":"5Hji1tsGC7OI"},"source":["#Colaboratory環境の設定\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/MathProgramming/Chapter9"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4Sko5mnZ0lR"},"source":["#ライブラリの設定\n","!pip install -q -r ./requirements2.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JozEFkWkTija"},"source":["#audio_dataset_3classファイルが解凍されていない場合コメントアウトを外して実行して下さい\n","#!unzip audio_dataset_3class.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tdd6hgiCbpo0"},"source":["## 9-6 音の分類のためにデータを前処理してみよう"]},{"cell_type":"code","metadata":{"id":"d4Cvr93dAoH_"},"source":["import librosa\n","import pandas as pd\n","import numpy as np\n","import IPython.display as ipd\n","\n","#学習データの読み込み\n","train_data_dir =\"./audio_dataset_3class/train/\"\n","train_df = pd.read_csv(\"audio_dataset_3class/train.csv\", index_col=0)\n","\n","#テストデータの読み込み\n","test_data_dir =\"./audio_dataset_3class/test/\"\n","test_df = pd.read_csv(\"audio_dataset_3class/test.csv\", index_col=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcoit0bXfPF4"},"source":["#学習に使うデータの音声ファイル名とラベルの1部を表示する\n","train_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbUBHqFyDHXf"},"source":["#ラベルに使われている値とその数の一覧を表示\n","train_df[\"label\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bqBEK-TKDMrY"},"source":["#チェロの音声データのうちの一つを読み込む\n","data, rate = librosa.load(train_data_dir+ train_df[train_df[\"label\"] == \"Cello\"].index[0])\n","\n","#読み込んだデータを再生する\n","ipd.Audio(data = data, rate = rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNWBzq5ON1Ze"},"source":["#読み込んだチェロの音声データの形を確認\n","print(data.shape)\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4AVMs85EELc"},"source":["from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","sampling_rate = 8000\n","#音の長さを3秒に切り取る\n","audio_duration = 3\n","audio_length = sampling_rate * audio_duration\n","\n","#ファイル名から音声データを読み込む\n","def _load_files(data_dir, filenames):\n","  result = []\n","  for i, filename in enumerate(filenames):\n","        file_path = data_dir + filename\n","        data, _ = librosa.core.load(file_path, sr=sampling_rate, res_type='kaiser_fast')\n","        result.append(data)\n","  \n","  return result\n","\n","\n","def create_audio_dataset(train_df, test_df, train_data_dir, test_data_dir, label_dict):\n","    \n","    dim = (audio_length, 1)\n","    train_filenames = train_df.index\n","    test_filenames = test_df.index\n","\n","    #学習用データとテスト用データの音声ファイルをファイル名から読み込む\n","    _X_train = _load_files(train_data_dir, train_filenames)\n","    _X_test = _load_files(test_data_dir, test_filenames)\n","    \n","    #audio_length(ここでは3秒)に音の長さを揃える\n","    _X_train = pad_sequences(_X_train, dtype='float32', maxlen=audio_length, padding='pre', truncating='pre', value=0.0).tolist()\n","    _X_test = pad_sequences(_X_test, dtype='float32', maxlen=audio_length, padding='pre', truncating='pre', value=0.0).tolist()\n","    \n","    #音のデータをStandardScalerで平均値を0、分散を１に補正する\n","    scaler = StandardScaler()\n","    scaler = scaler.fit(_X_train + _X_test)\n","    _X_train = scaler.transform(_X_train)\n","    _X_test = scaler.transform(_X_test)\n","\n","    X_train = np.empty((len(train_filenames), *dim))\n","    for index, data in enumerate(_X_train):\n","      X_train[index,] = [[d] for d in data]\n","    \n","    X_test = np.empty((len(test_filenames), *dim))\n","    for index, data in enumerate(_X_test):\n","      X_test[index,] = [[d] for d in data]\n","    \n","\n","    #以下からはlabelの作成\n","    labels_train = train_df[\"label\"]\n","    labels_test = test_df[\"label\"]\n","\n","    y_train = np.empty(len(labels_train), dtype=int)\n","    for i, label in enumerate(labels_train):\n","        y_train[i] = label_dict[label]\n","\n","    y_test = np.empty(len(labels_test), dtype=int)\n","    for i, label in enumerate(labels_test):\n","        y_test[i] = label_dict[label]\n","    \n","    #one-hot encodingする\n","    Y_train = to_categorical(y_train, num_classes=len(label_dict))\n","    Y_test = to_categorical(y_test, num_classes=len(label_dict))\n","\n","    return X_train, Y_train, X_test, Y_test\n","    \n","\n","audio_label_dict = {\"Cello\": 0,\"Clarinet\":1, \"Applause\":2}\n","X_train, Y_train, X_test, Y_test = create_audio_dataset(train_df, test_df, train_data_dir, test_data_dir, audio_label_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iPfhwLG4jBtp"},"source":["## 9-7 LSTMで音の分類をしてみよう"]},{"cell_type":"code","metadata":{"id":"6MK9Yg-kNrJO"},"source":["from tensorflow.keras.layers import Dense, LSTM, Dropout,Bidirectional\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","\n","def create_lstm_model():\n","  input_shape = (audio_length, 1)\n","  \n","  #モデルの構築\n","  model_lstm = Sequential()\n","  model_lstm.add(LSTM(64, return_sequences=True, dropout=0.3 ,input_shape=input_shape))\n","  model_lstm.add(LSTM(64, return_sequences=False, dropout=0.3))\n","  model_lstm.add(Dense(units=len(audio_label_dict), activation=\"softmax\"))\n","  model_lstm.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"acc\"])\n","  return model_lstm\n","\n","model_lstm = create_lstm_model()\n","#モデルの構造を表示する\n","model_lstm.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YlwcdkcBNrLv"},"source":["#学習開始\n","history = model_lstm.fit(X_train, Y_train, batch_size=16, epochs=40, validation_split=0.1, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0tuJWaMq4ee"},"source":["#学習に時間がかかるため、以下のコードのコメントアウトを外すことでモデルの重みの保存と読み込みが出来ます.\n","#１度保存しておき、8節以降をあとから実行する場合保存したモデルの重みを読み込むことで学習を再実行しなくて済みます\n","\n","#モデルの重みの保存\n","#model_lstm.save_weights('./saved_models/model_lstm_weights')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1zpXDcC-_1C"},"source":["#保存したモデルの読み込み\n","#model_lstm = create_lstm_model()\n","#model_lstm.load_weights('./saved_models/model_lstm_weights')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hRWvYKwlxeW"},"source":["## 9-8 LSTMで分類した結果を評価してみよう"]},{"cell_type":"code","metadata":{"id":"n1pvtUCqm4k8"},"source":["#予測開始\n","predictions = model_lstm.predict(X_test, verbose=1)\n","pred_labels = np.array([np.argmax(pred) for pred in predictions])\n","actual_labels = np.array([audio_label_dict[lab] for lab in test_df[\"label\"]])\n","\n","#正答率の算出\n","tmp = actual_labels == pred_labels\n","tmp.sum()/len(tmp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHGIAhKmow-Q"},"source":["import matplotlib.pyplot as plt\n","\n","#評価関数と精度のグラフ表示\n","fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history[\"loss\"], color=\"b\", label=\"Training Loss\")\n","ax[0].plot(history.history[\"val_loss\"], color=\"g\", label=\"Validation Loss\")\n","ax[0].legend()\n","\n","ax[1].plot(history.history[\"acc\"], color=\"b\", label=\"Training Accuracy\")\n","ax[1].plot(history.history[\"val_acc\"], color=\"g\", label=\"Validation Accuracy\")\n","ax[1].legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LHCjss3lqjq9"},"source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","#混同行列の作成\n","cf_matrix = confusion_matrix(actual_labels, pred_labels)\n","\n","plt.figure(figsize=(13,13))\n","c = sns.heatmap(cf_matrix, annot=True, fmt=\"d\")\n","\n","#audio_label_dict = {\"Cello\": 0,\"Clarinet\":1, \"Applause\":2}\n","audio_label_list = [\"Cello\", \"Clarinet\", \"Applause\"]\n","c.set(xticklabels=audio_label_list, yticklabels=audio_label_list)\n","plt.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tLIFyMYHNsWc"},"source":["## 9-9 CNNで音の分類をしてみよう"]},{"cell_type":"code","metadata":{"id":"Arehnp7nHXTi"},"source":["from tensorflow.keras.layers import Activation, Conv1D, MaxPooling1D, GlobalMaxPool1D,Dropout\n","\n","def create_cnn_model():\n","  #モデルの構築\n","  input_shape = (audio_length, 1)\n","  model_cnn = Sequential()    \n","  model_cnn.add(Conv1D(filters=128, kernel_size=9, padding='valid', input_shape=input_shape, activation='relu'))\n","  model_cnn.add(MaxPooling1D(pool_size=16))\n","  model_cnn.add(Dropout(rate=0.2))\n","  model_cnn.add(Conv1D(filters=64, kernel_size=3, padding='valid', activation='relu'))\n","  model_cnn.add(GlobalMaxPool1D())\n","  model_cnn.add(Dropout(rate=0.2))\n","  model_cnn.add(Dense(len(audio_label_dict), activation=\"softmax\"))\n","  model_cnn.compile(optimizer=Adam(0.0001), loss=\"categorical_crossentropy\", metrics=['acc'])\n","  return model_cnn\n","\n","model_cnn = create_cnn_model()\n","#モデルの構造を表示する\n","model_cnn.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsqUBflzJGWy"},"source":["history = model_cnn.fit(X_train, Y_train, batch_size=16, epochs=50, validation_split=0.1, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RjokKOkK21mw"},"source":["## 9-10 CNNで分類した結果を評価してみよう"]},{"cell_type":"code","metadata":{"id":"OdA3Jw2q283d"},"source":["#予測開始\n","predictions = model_cnn.predict(X_test, verbose=1)\n","pred_labels = np.array([np.argmax(pred) for pred in predictions])\n","actual_labels = np.array([audio_label_dict[lab] for lab in test_df[\"label\"]])\n","\n","#正答率の算出\n","tmp = actual_labels == pred_labels\n","tmp.sum()/len(tmp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4uueWj3JG7z"},"source":["#評価関数と精度のグラフ表示\n","fig, ax = plt.subplots(2,1)\n","ax[0].plot(history.history[\"loss\"], color=\"b\", label=\"Training Loss\")\n","ax[0].plot(history.history[\"val_loss\"], color=\"g\", label=\"Validation Loss\")\n","ax[0].legend()\n","\n","ax[1].plot(history.history[\"acc\"], color=\"b\", label=\"Training Accuracy\")\n","ax[1].plot(history.history[\"val_acc\"], color=\"g\", label=\"Validation Accuracy\")\n","ax[1].legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrS0GRAZJyLb"},"source":["#混同行列の作成\n","cf_matrix = confusion_matrix(actual_labels, pred_labels)\n","\n","plt.figure(figsize=(13,13))\n","c = sns.heatmap(cf_matrix, annot=True, fmt=\"d\")\n","\n","#audio_label_dict = {\"Cello\": 0,\"Clarinet\":1, \"Applause\":2}\n","audio_label_list = [\"Cello\", \"Clarinet\", \"Applause\"]\n","c.set(xticklabels=audio_label_list, yticklabels=audio_label_list)\n","plt.plot()"],"execution_count":null,"outputs":[]}]}