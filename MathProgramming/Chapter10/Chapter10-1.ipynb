{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10章_深層学習によって実現できる画像処理・言語処理を知ろう(1-4節).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UnzfHU4zdYYa"},"source":["# 第10章 深層学習によって実現できる画像処理・言語処理を知ろう(1-4節)\n","ここでは、深層学習を使った物体検出アルゴリズムについて学んでいきます。\n","\n","Google Colaboratory上で実行する場合、ランタイムがGPUになっていることを確認して下さい"]},{"cell_type":"code","metadata":{"id":"kqHgOuSaNA8u"},"source":["#Colaboratory環境の設定\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/MathProgramming/Chapter10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qhWiI1G1CzJM"},"source":["#ライブラリの設定\n","!pip install -q -r ./requirements1.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGLS2V67JR-f"},"source":["## 10-3 YOLOを用いて物体検出を行ってみよう"]},{"cell_type":"code","metadata":{"id":"g_sBaX_5nJBz"},"source":["#データセットのダウンロード及び解凍を行います。\n","#ダウンロード済みでない場合以下を実行して下さい。\n","!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n","!tar -xvf ./VOCtrainval_06-Nov-2007.tar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAKvqj2XnQj-"},"source":["#yolov3-tf2のダウンロード\n","!git clone https://github.com/zzh8829/yolov3-tf2.git ./yolov3_tf2\n","%cd ./yolov3_tf2\n","!git checkout c43df87d8582699aea8e9768b4ebe8d7fe1c6b4c\n","%cd ../"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5BjFMFnn7Za"},"source":["#YOLOの学習済みモデルのダウンロード\n","!wget https://pjreddie.com/media/files/yolov3-tiny.weights "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eByPqawwoVc7"},"source":["#ダウンロードしたYOLOの学習済みモデルをKerasから利用出来る形に変換\n","!python ./yolov3_tf2/convert.py --weights ./yolov3-tiny.weights --output  ./yolov3_tf2/checkpoints/yolov3-tiny.tf --tiny"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZRSb05V_ryGC"},"source":["from PIL import Image\n","\n","#ダウンロードしたデータセットの画像の内１枚を表示\n","Image.open(\"./VOCdevkit/VOC2007/JPEGImages/006626.jpg\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7-aAtnWryIi"},"source":["#表示した画像のアノテーションデータの表示\n","annotation = open(\"./VOCdevkit/VOC2007/Annotations/006626.xml\").read()\n","print(annotation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jLnLblWMqm2p"},"source":["import xmltodict\n","import numpy as np\n","from tensorflow.keras.utils import Sequence\n","import math\n","import yolov3_tf2.yolov3_tf2.dataset as dataset\n","\n","yolo_max_boxes = 100\n","\n","#アノテーションデータの変換\n","def parse_annotation(annotation, class_map):\n","    label = []\n","    width = int(annotation['size']['width'])\n","    height = int(annotation['size']['height'])\n","    \n","    if 'object' in annotation:\n","        if type(annotation['object']) != list:\n","            tmp = [annotation['object']]\n","        else:\n","            tmp = annotation['object']\n","            \n","        for obj in tmp:\n","            _tmp = []\n","            _tmp.append(float(obj['bndbox']['xmin']) / width)\n","            _tmp.append(float(obj['bndbox']['ymin']) / height)\n","            _tmp.append(float(obj['bndbox']['xmax']) / width)\n","            _tmp.append(float(obj['bndbox']['ymax']) / height)\n","            _tmp.append(class_map[obj['name']])\n","            label.append(_tmp)\n","\n","    for _ in range(yolo_max_boxes - len(label)):\n","      label.append([0,0,0,0,0])\n","    return label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlyQ5j_PoXPa"},"source":["from yolov3_tf2.yolov3_tf2.dataset import transform_images\n","\n","#学習時に画像データを必要な分だけ読み込むためのクラス\n","class ImageDataSequence(Sequence):\n","    def __init__(self, file_name_list, batch_size,  anchors, anchor_masks, class_names, data_shape=(256,256,3)):\n","        \n","        #クラス名とそれに対応する数値、という形の辞書を作る\n","        self.class_map = {name: idx for idx, name in enumerate(class_names)}\n","        self.file_name_list = file_name_list\n","\n","        self.image_file_name_list = [\"./VOCdevkit/VOC2007/JPEGImages/\"+image_path + \".jpg\" for image_path in self.file_name_list]\n","        self.annotation_file_name_list = ['./VOCdevkit/VOC2007/Annotations/' + image_path+ \".xml\" for image_path in self.file_name_list]\n","\n","        self.length = len(self.file_name_list)\n","        self.data_shape = data_shape\n","        self.batch_size = batch_size\n","        self.anchors = anchors\n","        self.anchor_masks = anchor_masks\n","\n","        self.labels_cache = [None for i in range(self.__len__())]\n","\n","    #１バッチごとに自動的に呼ばれる。画像データとそのラベルを必要な分だけ読み込んで返す\n","    def __getitem__(self, idx):\n","        images = []\n","        labels = []\n","        \n","        #現在のバッチが何回目か、がidx変数に入っているため、それに対応するデータを読み込む\n","        for index in range(idx*self.batch_size, (idx+1)*self.batch_size):\n","\n","          #アノテーションデータをラベルとして使える形に変換する\n","          annotation = xmltodict.parse((open(self.annotation_file_name_list[index]).read()))\n","          label = parse_annotation(annotation[\"annotation\"], self.class_map)\n","          labels.append(label)\n","\n","          #画像データの読み込みと加工\n","          img_raw = tf.image.decode_jpeg(open(self.image_file_name_list[index], 'rb').read(), channels=3)\n","          img = transform_images(img_raw, self.data_shape[0])\n","          images.append(img)\n","        \n","        #ラベルに対しても前処理をするが、時間がかかるため１度読み込んだらキャッシュとして保存する\n","        if self.labels_cache[idx] is None:\n","          labels = tf.convert_to_tensor(labels, tf.float32)\n","          labels = dataset.transform_targets(labels, self.anchors, self.anchor_masks, self.data_shape[0])\n","          self.labels_cache[idx] = labels\n","        else: \n","          labels = self.labels_cache[idx]\n","\n","        images = np.array(images)\n","        return images, labels\n","\n","    def __len__(self):\n","        return math.floor(len(self.file_name_list) / self.batch_size)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pT83GAyExapw"},"source":["from  yolov3_tf2.yolov3_tf2.models import  YoloV3Tiny, YoloLoss\n","from yolov3_tf2.yolov3_tf2.utils import freeze_all\n","import tensorflow as tf\n","\n","batch_size=16\n","data_shape=(416,416,3)\n","class_names =  [\"person\", \"bird\", \"cat\",\"cow\",\"dog\", \"horse\",\"sheep\", \"aeroplane\", \"bicycle\", \"boat\", \"bus\", \"car\", \"motorbike\", \"train\", \"bottle\", \"chair\", \"diningtable\", \"pottedplant\", \"sofa\", \"tvmonitor\"]\n","\n","anchors = np.array([(10, 14), (23, 27), (37, 58),\n","                              (81, 82), (135, 169),  (344, 319)],\n","                             np.float32) / data_shape[0]\n","anchor_masks = np.array([[3, 4, 5], [0, 1, 2]])\n","\n","# yolov3_tf2で定義されているtiny YOLOのモデルを読み込む\n","model_pretrained = YoloV3Tiny(data_shape[0], training=True, classes=80)\n","model_pretrained.load_weights(\"./yolov3_tf2/checkpoints/yolov3-tiny.tf\").expect_partial()\n","\n","model = YoloV3Tiny(data_shape[0], training=True, classes=len(class_names))\n","#ここで、学習済みモデルの出力層以外の重みだけを取り出す\n","model.get_layer('yolo_darknet').set_weights(model_pretrained.get_layer('yolo_darknet').get_weights())\n","#出力層以外を学習しないようにする\n","freeze_all(model.get_layer('yolo_darknet'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7uv8wOy-yJfO"},"source":["loss = [YoloLoss(anchors[mask], classes=len(class_names)) for mask in anchor_masks]\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss=loss, run_eagerly=False)\n","\n","#モデルの構造を出力\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5xgUKjiyRSQ"},"source":["train_file_name_list = open(\"./VOCdevkit/VOC2007/ImageSets/Main/train.txt\").read().splitlines()\n","validation_file_name_list = open(\"./VOCdevkit/VOC2007/ImageSets/Main/val.txt\").read().splitlines()\n","\n","train_dataset = ImageDataSequence(train_file_name_list, batch_size, anchors, anchor_masks, class_names, data_shape=data_shape)\n","validation_dataset = ImageDataSequence(validation_file_name_list, batch_size, anchors, anchor_masks, class_names, data_shape=data_shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQrv-F_YyW6f"},"source":["history = model.fit(train_dataset, validation_data=validation_dataset, epochs=30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yHdcPhz_yZXU"},"source":["#学習した重みの保存\n","model.save_weights('./saved_models/model_yolo_weights')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3OqkGbOW3c6i"},"source":["## 10-4 物体検出を行った結果を評価してみよう"]},{"cell_type":"code","metadata":{"id":"_Hp9rZ5q1Iay"},"source":["from absl import app, logging, flags\n","from absl.flags import FLAGS\n","app._run_init(['yolov3'], app.parse_flags_with_usage)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XizpUkI11IdK"},"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from yolov3_tf2.yolov3_tf2.utils import draw_outputs\n","\n","yolo_trained = YoloV3Tiny(classes=len(class_names))\n","#保存した重みの読み込み\n","yolo_trained.load_weights('./saved_models/model_yolo_weights').expect_partial()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"smr62Jkw1Ifq"},"source":["img_file_name = \"./VOCdevkit/VOC2007/JPEGImages/\"+\"006626\" + \".jpg\"\n","\n","#画像の読み込み\n","img_raw = tf.image.decode_jpeg(open(img_file_name, 'rb').read(), channels=3)\n","img = transform_images(img_raw, data_shape[0])\n","img = np.expand_dims(img, 0)\n","\n","\n","#予測開始\n","boxes, scores, classes, nums = yolo_trained.predict(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKiCvBwC1Ih4"},"source":["img = img_raw.numpy()\n","\n","#予測結果を画像に書き込み\n","img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n","\n","#予測結果を書き込んだ画像の表示\n","plt.figure(figsize=(10,10))\n","plt.imshow(img)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2bBDwf61Ip8"},"source":["#学習済みの重みをそのまま利用する場合\n","\n","FLAGS.yolo_iou_threshold = 0.5\n","FLAGS.yolo_score_threshold = 0.5\n","\n","yolo_class_names = [c.strip() for c in open(\"./yolov3_tf2/data/coco.names\").readlines()]\n","\n","yolo = YoloV3Tiny(classes=80)\n","#重みの読み込み\n","yolo.load_weights(\"./yolov3_tf2/checkpoints/yolov3-tiny.tf\").expect_partial()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0_NTn0P_VWC"},"source":["img_file_name = \"./VOCdevkit/VOC2007/JPEGImages/\"+\"006626\" + \".jpg\"\n","\n","img_raw = tf.image.decode_jpeg(open(img_file_name, 'rb').read(), channels=3)\n","img = transform_images(img_raw, data_shape[0])\n","img = np.expand_dims(img, 0)\n","#予測開始\n","boxes, scores, classes, nums = yolo.predict(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpFo8yPt_dte"},"source":["img = img_raw.numpy()\n","img = draw_outputs(img, (boxes, scores, classes, nums), yolo_class_names)\n","\n","plt.figure(figsize=(10,10))\n","plt.imshow(img)\n","plt.show()"],"execution_count":null,"outputs":[]}]}