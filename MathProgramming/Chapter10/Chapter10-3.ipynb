{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter10-3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lozOstEFZjwY"},"source":["# 第10章 深層学習によって実現できる画像処理・言語処理を知ろう(8-10節)\n","ここでは、深層学習を使った自然言語処理について学んでいきます。\n","\n","Google Colaboratory上で実行する場合、ランタイムがGPUになっていることを確認して下さい"]},{"cell_type":"code","metadata":{"id":"sUKKo31dLSIq"},"source":["#Colaboratory環境の設定\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/MathProgramming/Chapter10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MoIRAnPQZvEt"},"source":["#ライブラリの設定\n","!pip install -q -r ./requirements3.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rKcM6jRoZyXL"},"source":["## 10-9 Bertで文書の分類をしてみよう"]},{"cell_type":"code","metadata":{"id":"AnZo7PBbZsoA"},"source":["import pandas as pd\n","data_file='./spam.csv'\n","df = pd.read_csv('./spam.csv')\n","print(df[\"label\"].value_counts())\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GycwhFqbZ3tM"},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","import numpy as np\n","tf.config.run_functions_eagerly(False)\n","\n","#前処理をするモジュールの読み込み\n","bert_preprocess = hub.load(\"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/2\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DmQ1xHekaO4R"},"source":["test_preprocessed = bert_preprocess([\"Hello World!\"])\n","test_preprocessed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQFLpq0Pads0"},"source":["#データを学習に７割り、テストに３割り使うように分ける\n","train_df = df[0: int(len(df)*0.7)]\n","test_df = df[int(len(df)*0.7):]\n","\n","#前処理を行うモジュールで文字列の処理\n","X_train =  bert_preprocess(train_df[\"text\"])\n","X_test = bert_preprocess(test_df[\"text\"])\n","\n","#ラベル(SpamとHam)をOnehot encoding\n","Y_train = pd.get_dummies(train_df[\"label\"]).values.astype(np.float32)\n","Y_test = pd.get_dummies(test_df[\"label\"]).values.astype(np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IBzxczfOvnR6"},"source":["#モデルの構築\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Input, Dense, Dropout\n","\n","#入力はinput_word_ids, input_mask, input_type_idsの３つ。\n","inputs = dict(\n","      input_word_ids=Input(shape=(None,), dtype=tf.int32),\n","      input_mask=Input(shape=(None,), dtype=tf.int32),\n","      input_type_ids=Input(shape=(None,), dtype=tf.int32))\n","\n","#Tensorflow HubよりBertのモデルを読み込む\n","outputs = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1\", trainable=True, name='bert_encoder')(inputs)\n","outputs = outputs[\"pooled_output\"]\n","outputs = Dropout(0.1)(outputs)\n","#最終的な出力は２つ（SpamとHam)になるように全結合層を最後に付ける\n","outputs = Dense(2, activation=\"softmax\", name='classifier')(outputs)\n","model = Model(inputs, outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yk5ETHwea0P5"},"source":["from official.nlp import optimization\n","EPOCHS = 3\n","num_train_steps =  len(train_df.index) * EPOCHS\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","#OptimizerとしてAdamWを利用\n","optimizer = optimization.create_optimizer(init_lr=0.00003,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')\n","\n","model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n","\n","#モデルの概要を出力\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_93_NFpY2Abo"},"source":["#学習開始\n","hist = model.fit(X_train,Y_train,epochs=EPOCHS, validation_split=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAgsbBbN3RRH"},"source":["#学習に時間がかかるため、以下のコードのコメントアウトを外すことでモデルの重みの保存と読み込みが出来ます.\n","#１度保存しておき、10節以降をあとから実行する場合保存したモデルの重みを読み込むことで学習を再実行しなくて済みます\n","\n","#学習した重みの保存\n","#model.save_weights('./saved_models/model_bert_weights')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmxfpBQm3nh-"},"source":["#重みの読み込み\n","#model.load_weights('./saved_models/model_bert_weights')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rY2paknfs2YE"},"source":["## 10-10 Bertを用いて分類した文章の評価をしてみよう\n","\n"]},{"cell_type":"code","metadata":{"id":"KE9cmUX7bknF"},"source":["#分類開始\n","pred = model.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SsCPvEgFjeEJ"},"source":["pred_labels = np.array([np.argmax(p) for p in pred])\n","actual_labels = np.array([np.argmax(t) for t in Y_test])\n","tmp = actual_labels == pred_labels\n","tmp.sum()/len(tmp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVFgjp0obolD"},"source":["#混同行列の表示\n","\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","cf_matrix = confusion_matrix(actual_labels, pred_labels)\n","\n","c = sns.heatmap(cf_matrix, annot=True, fmt=\"d\")\n","\n","label_dict = {\"ham\": 0, \"spam\":1}\n","c.set(xticklabels=label_dict, yticklabels=label_dict)\n","plt.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iar0zkAMhGlz"},"source":["#予測したテキスト\n","print(\"予測: \" , pred_labels[0])\n","print(test_df.iloc[0][\"text\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6QYBSkQhUrw"},"source":["print(\"予測: \" , pred_labels[3])\n","print(test_df.iloc[3][\"text\"])"],"execution_count":null,"outputs":[]}]}